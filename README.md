# Soton LM Data Engineering

[](https://www.python.org/downloads/)
[](https://dvc.org/)
[](https://duckdb.org/)
[](https://github.com/psf/black)
[](https://github.com/astral-sh/ruff)

## ğŸ¯ Project Overview

This is the data engineering division of the SotonLM project, tasked with building high-quality training data for large language models from diverse sources. Our mission is to transform raw, unstructured data from the internet into clean, structured, and version-controlled datasets ready for model training.

This repository contains all pipeline code, data pointers (DVC), and documentation.

## ğŸš€ Team Structure

We operate through three specialized divisions, each focused on a different domain of data:

| Division | Focus Area | Key Tools & Sources |
| :--- | :--- | :--- |
| **ğŸ”¬ Division 1 - Academic** | Research & Technical Content | `arxiv` (API), `PyMuPDF` (PDFs) |
| **ğŸŒ Division 2 - Web** | General Knowledge | `Scrapy`, `datasets` (Wikipedia), `BeautifulSoup4` (HTML) |
| **ğŸ’¬ Division 3 - Social** | Conversational Data | `PRAW` (Reddit), `Scrubadub` (PII Cleaning) |

## âš™ï¸ Quick Start

This guide will set up your local environment, connect you to the data, and get you ready to contribute.

### Prerequisites

  * Python 3.10 or higher
  * [Git](https://git-scm.com/)
  * [AWS CLI](https://aws.amazon.com/cli/) (for connecting to S3)
  * [DVC (Data Version Control)](https://dvc.org/doc/install)

### Setup Steps

1.  **Clone the Repository:**

    ```bash
    git clone https://github.com/SotonLM/Data-Engineering.git
    cd Data-Engineering
    ```

2.  **Create and Activate Virtual Environment:**

    ```bash
    # Create the environment
    python -m venv venv

    # Activate it (Mac/Linux)
    source venv/bin/activate

    # Or (Windows)
    .\venv\Scripts\activate
    ```

3.  **Install Dependencies:**
    We use `pyproject.toml` to define dependencies and `pip-tools` to compile `requirements.txt` files.

    ```bash
    # Install the core tools to get started
    pip install pip-tools

    # Compile and install the main dependencies
    pip-compile pyproject.toml -o requirements.txt
    pip install -r requirements.txt

    # Compile and install the development dependencies (like pytest, black, ruff)
    pip-compile pyproject.toml --extras dev -o requirements-dev.txt
    pip install -r requirements-dev.txt
    ```

4.  **Install Git Hooks:**
    This will automatically format and lint your code on every commit.

    ```bash
    pre-commit install
    ```

5.  **Configure AWS & DVC:**
    This connects you to the S3 bucket where the 100GB+ dataset is stored.

    ```bash
    # 1. Configure your AWS credentials (using the keys provided by your co-lead)
    aws configure

    # 2. Pull the data from DVC (this will download the current dataset)
    dvc pull
    ```

You are now fully set up and have the latest version of both the code and the data.

## ğŸ“ Project Structure

```text
soton-lm-data-engineering/
â”œâ”€â”€ .dvc/                 # DVC internal files
â”œâ”€â”€ .github/              # GitHub workflows and PR templates
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # Raw source data (tracked by DVC)
â”‚   â”‚   â”œâ”€â”€ division_1_academic/
â”‚   â”‚   â”œâ”€â”€ division_2_web/
â”‚   â”‚   â””â”€â”€ division_3_social/
â”‚   â””â”€â”€ clean/            # Processed, clean data (tracked by DVC)
â”‚       â””â”€â”€ ...
â”œâ”€â”€ docs/                 # Documentation (e.g., complete_documentation.json)
â”œâ”€â”€ src/                  # Source code for all data pipelines
â”‚   â”œâ”€â”€ division_1_academic/
â”‚   â”œâ”€â”€ division_2_web/
â”‚   â”œâ”€â”€ division_3_social/
â”‚   â””â”€â”€ shared/           # Common utilities (e.g., deduplication)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ pyproject.toml        # Project definition and dependencies
â”œâ”€â”€ requirements.txt      # Main locked dependencies (auto-generated)
â””â”€â”€ requirements-dev.txt  # Dev locked dependencies (auto-generated)
```

